{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 1\n",
    "\n",
    "Submitted by: Arjun Varma, Nimish Amlathe, daniel Peng, Ann Li, Rachel Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import operator\n",
    "import re,string\n",
    "from patsy import dmatrices\n",
    "%pylab inline\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. What are the top 5 parts of speech in this corpus of job descriptions? How frequently do they appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Train_rev1.csv')\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running only for 10,000 rows to reduce runtime\n",
    "description  = train['FullDescription'][:1000]\n",
    "\n",
    "corpus = description.str.cat(sep=' ')\n",
    "\n",
    "corpus = corpus.decode('utf-8')\n",
    "\n",
    "# Converting to lower case and making sure there are only alphabetic characters\n",
    "corpus_words = nltk.word_tokenize(corpus.lower())\n",
    "\n",
    "corpus_words = [word for word in corpus_words if word.isalpha()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Calculate the frequencies of PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'engineering', 'NN'),\n",
       " (u'systems', 'NNS'),\n",
       " (u'analyst', 'NN'),\n",
       " (u'dorking', 'VBG'),\n",
       " (u'surrey', 'JJ'),\n",
       " (u'salary', 'JJ'),\n",
       " (u'our', 'PRP$'),\n",
       " (u'client', 'NN'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'located', 'VBN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(corpus_words)\n",
    "\n",
    "pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\Arjun\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:7: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>51790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>23246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>23123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>17170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>15271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word\n",
       "pos       \n",
       "NN   51790\n",
       "JJ   23246\n",
       "IN   23123\n",
       "DT   17170\n",
       "NNS  15271"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq = sorted(pos, key=lambda x: x[1],reverse=True)\n",
    "\n",
    "df_pos = pd.DataFrame(pos_freq)\n",
    "\n",
    "df_pos.columns = ['word','pos']\n",
    "\n",
    "df_pos.groupby('pos').count().sort('word',ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Does this corpus support Zipf’s law? Plot the most common 100 words in the corpus against the theoretical prediction of the law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Calculate the frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>8435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>7121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>6677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>5812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>4229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for</td>\n",
       "      <td>3450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>care</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>you</td>\n",
       "      <td>2469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  freq\n",
       "0   and  8435\n",
       "1   the  7121\n",
       "2    to  6677\n",
       "3     a  5812\n",
       "4    of  4547\n",
       "5    in  4229\n",
       "6   for  3450\n",
       "7  care  3033\n",
       "8  with  2652\n",
       "9   you  2469"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sorted = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "df = pd.DataFrame.from_records(c_sorted)\n",
    "df.columns = ['word','freq']\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Get rid of the punctuation marks and then rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = ~df['word'].isin(set(string.punctuation)) \n",
    "\n",
    "df = df[mask]\n",
    "\n",
    "df['rank'] = df['freq'].rank(method='min',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>8435</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>7121</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>6677</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>5812</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4547</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  freq  rank\n",
       "0  and  8435   1.0\n",
       "1  the  7121   2.0\n",
       "2   to  6677   3.0\n",
       "3    a  5812   4.0\n",
       "4   of  4547   5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5][['word','freq','rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Plot the most common 100 words in the corpus against the theoretical prediction of the law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\Arjun\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x20ac5438>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCUBEJKH+PwtzElEwhp9qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6fbaac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_100 = df[:100]\n",
    "\n",
    "# Theoretical value of Zipf's law\n",
    "df_100['theoretical']= [df[\"freq\"].max()/c for c in df_100['rank']]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actuals vs theoretical\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = plt.plot([math.log(c) for c in df_100['rank'].values], [math.log(c) for c in df_100['freq']], 'ro',color='black')\n",
    "\n",
    "ax2 = plt.plot([math.log(c) for c in df_100['rank'].values], [math.log(c) for c in df_100['theoretical']])\n",
    "\n",
    "xlabel(\"log(rank)\")\n",
    "ylabel(\"log(frequency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus does follow Zipf's law but the words with the higher frequencies do not follow the law as closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. If we remove stopwords and lemmatize the corpus, what are the 10 most common words? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'care', 3152),\n",
       " (u'work', 2169),\n",
       " (u'home', 2117),\n",
       " (u'experience', 1675),\n",
       " (u'nurse', 1505),\n",
       " (u'manager', 1276),\n",
       " (u'support', 1275),\n",
       " (u'within', 1040),\n",
       " (u'nursing', 941),\n",
       " (u'job', 941)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Removing punctuations and stop words\n",
    "corpus_words= [word for word in corpus_words if word not in set(string.punctuation)]\n",
    "filtered_words_sw = [word for word in corpus_words if word not in stopwords.words('english')]\n",
    "\n",
    "# Lemmatizing based on the POS\n",
    "filtered_poi = nltk.pos_tag(filtered_words_sw)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "wordnet_tag ={'NN':'n','JJ':'a','VB':'v','RB':'r'}\n",
    "words_lem = []\n",
    "for t in filtered_poi:\n",
    "    try: words_lem.append(wnl.lemmatize(t[0],wordnet_tag[t[1][:2]]))\n",
    "    except: words_lem.append(wnl.lemmatize(t[0]))\n",
    "\n",
    "# Getting most common words\n",
    "fdist_2 = nltk.FreqDist(words_lem)\n",
    "fdist_2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## B1 Create a classification model with all words and the bag-of-words approach. How accurate is the model (show the confusion matrix)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that counts the frequency of each token in a document\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in corpus_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the cutoff salary\n",
    "cutoff = np.percentile(train[['SalaryNormalized']], 75)\n",
    "train['category'] = 'high'\n",
    "train.ix[train.SalaryNormalized < cutoff, 'category'] = 'low'\n",
    "\n",
    "train['token'] = \"\"\n",
    "fullset = train[['FullDescription','token','category']][:1000]\n",
    "\n",
    "# Calculate the document_features in each document\n",
    "from __future__ import unicode_literals\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    token = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    token = [w for w in token if w.isalpha()==True]\n",
    "    feature = document_features(token)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the feature set and run the model\n",
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  0.822\n"
     ]
    }
   ],
   "source": [
    "print 'The accuracy is ', nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.6%>  8.6% |\n",
      "high |   9.2%  <1.6%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2 . Speculate before running the following analysis whether lemmatization would help improve the accuracy of classification. Now create a classification model after lemmatization. Did the classification accuracy increase relative to B1? Comment on your speculation versus the actual results you obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization should improve the accuracy since there will be fewer tokens with less noise. With each token appearing in a document, the likelihood of the document being positive or negative should increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatize words according to their pos in the corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    token = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    pos_i = nltk.pos_tag(token)\n",
    "    wordnet_tag ={'NN':'n','JJ':'a','VB':'v','RB':'r'}\n",
    "    resultList = []\n",
    "    for t in pos_i:\n",
    "        try: resultList.append(wnl.lemmatize(t[0],wordnet_tag[t[1][:2]]))\n",
    "        except: resultList.append(wnl.lemmatize(t[0]))\n",
    "    feature = document_features(resultList)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run classification model\n",
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy:  0.82\n"
     ]
    }
   ],
   "source": [
    "print 'The accuracy: ', nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.8%>  8.4% |\n",
      "high |   9.6%  <1.2%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After lemmatization, the accuracy actually went down slightly. Contrary to what we expected, lemmatization has minimal impact on the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Now speculate whether stopwords removal from the original data would help increase the accuracy of the model. Take out the stopwords (but do not lemmatize), build a classification model and check the accuracy, and compare with that in B1 & B2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word removal should not affect the accuracy of our model in terms of predictive power. So, we think that they will not help in categorizing documents as they will occur in both classes randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the data and removing stop words\n",
    "from __future__ import unicode_literals\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", words)\n",
    "    words = words.lower().split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    clean_data = \" \".join(meaningful_words)\n",
    "    token = nltk.word_tokenize(clean_data.lower().decode('utf-8'))\n",
    "    feature = document_features(token)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy:  0.82\n"
     ]
    }
   ],
   "source": [
    "# Creating the feature sets and running the classifier\n",
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print 'The accuracy: ',nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      contains(regional) = True             high : low    =     20.9 : 1.0\n",
      "  contains(relationship) = True             high : low    =     20.9 : 1.0\n",
      " contains(profitability) = True             high : low    =     20.9 : 1.0\n",
      " contains(presentations) = True             high : low    =     18.1 : 1.0\n",
      "    contains(governance) = True             high : low    =     18.1 : 1.0\n",
      "        contains(doctor) = True             high : low    =     16.3 : 1.0\n",
      "contains(decommissioning) = True             high : low    =     16.3 : 1.0\n",
      "         contains(units) = True             high : low    =     16.3 : 1.0\n",
      "     contains(inservice) = True             high : low    =     16.3 : 1.0\n",
      "    contains(beneficial) = True             high : low    =     16.3 : 1.0\n",
      "None\n",
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <80.6%>  8.6% |\n",
      "high |   9.4%  <1.4%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classifier.show_most_informative_features(10)\n",
    "\n",
    "# Confusion matrix\n",
    "test_set_mod = [x[0] for x in test_set]\n",
    "\n",
    "predicted_categories_NaiveBayes = classifier.classify_many(test_set_mod)\n",
    "actual_categories = [x[1] for x in test_set]\n",
    "\n",
    "cm = nltk.ConfusionMatrix(actual_categories, predicted_categories_NaiveBayes)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word removal did not help with performance. The accuracy is similar to what we obtained from the earlier models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4 .  Use the job descriptions without lemmatiztion and stopword removal. Add parts-of-speech bigrams to the bag-of-words, and run a new classification model. Does the accuracy increase over the results in B1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add parts-of-speech bigrams to the bag-of-words\n",
    "pos_bigram = list(nltk.bigrams(pos))\n",
    "pattern = [('JJ', 'NN'),('JJ','NNS'),('NN','NNS')]\n",
    "import copy\n",
    "corpus_words_bi = copy.deepcopy(corpus_words)\n",
    "\n",
    "for i in range(len(pos_bigram)):\n",
    "    (a,b),(c,d) = pos_bigram[i]\n",
    "    if (b,d) in pattern:\n",
    "        corpus_words_bi.append(str(a)+' '+str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function to create new features\n",
    "def document_features_bigram(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for wd in corpus_words_bi:\n",
    "        features['contains({})'.format(wd)] = (wd in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find out the unigrams and bigrams in each job description that match the part-of-speech patterns we defined\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    tokens = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    tokens = [w for w in tokens if w.isalpha()==True]\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    bigram = list(nltk.bigrams(tags))\n",
    "    for n in range(len(bigram)):\n",
    "        (a,b),(c,d) = bigram[n]\n",
    "        if (b,d) in pattern:\n",
    "            tokens.append(str(a)+' '+str(c))\n",
    "    feature = document_features_bigram(tokens)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy:  0.81\n"
     ]
    }
   ],
   "source": [
    "# Run classification model\n",
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print 'The accuracy: ',nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             h |\n",
      "     |      l      i |\n",
      "     |      o      g |\n",
      "     |      w      h |\n",
      "-----+---------------+\n",
      " low | <79.6%>  9.6% |\n",
      "high |   9.4%  <1.4%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "test_mod = [x[0] for x in test_set]\n",
    "predict = classifier.classify_many(test_mod)\n",
    "actual = [x[1] for x in test_set]\n",
    "cm = nltk.ConfusionMatrix(actual, predict)\n",
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite adding bigrams which we speculated will have more predictive power than single words, the accuracy of the model did not improve when compared to B1."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
